{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 a \n",
    "## Chinagorom Mbaraonye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T13:49:36.547390Z",
     "start_time": "2020-08-04T13:49:36.371360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _cs_matrix.getnnz of <6936x165 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 62424 stored elements in Compressed Sparse Row format>>\n"
     ]
    }
   ],
   "source": [
    "dir = \"C:/Users/Chinagorom Mbaraonye/Downloads\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#read in the file and make a copy of the dataset\n",
    "csv_path = os.path.join(dir, \"googleplaystore.csv\")\n",
    "apps = pd.read_csv(csv_path)\n",
    "dat = apps.copy()\n",
    "\n",
    "#separate features from labels\n",
    "y = dat[\"Installs\"]\n",
    "X = dat.drop(\"Installs\", axis=1)\n",
    "\n",
    "classnames, indices = np.unique(y, return_inverse=True)\n",
    "y = indices\n",
    "\n",
    "#first split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X,y, test_size=0.2, random_state=34, \n",
    "                                                              stratify=y)\n",
    "\n",
    "#now split the train_full into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full,y_train_full, test_size=0.2, random_state=34, \n",
    "                                                              stratify=y_train_full)\n",
    "\n",
    "#separate numeric from categorical features\n",
    "X_num = X.select_dtypes(include=[np.number]) \n",
    "X_cat = X.select_dtypes(exclude=[np.number])\n",
    "\n",
    "#build pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_attribs = list(X_num)\n",
    "\n",
    "cat_classes = np.unique(dat[\"Category\"])\n",
    "type_classes = np.unique(dat[\"Type\"])\n",
    "cont_classes = np.unique(dat[\"Content Rating\"])\n",
    "gen_classes = np.unique(dat[\"Genres\"])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat1\", OneHotEncoder(categories=[cat_classes]), [\"Category\"]),\n",
    "        (\"cat2\", OneHotEncoder(categories=[type_classes]), [\"Type\"]),\n",
    "        (\"cat3\", OneHotEncoder(categories=[cont_classes]), [\"Content Rating\"]),\n",
    "        (\"cat4\", OneHotEncoder(categories=[gen_classes]), [\"Genres\"])\n",
    "    ])\n",
    "\n",
    "X_train_prep = full_pipeline.fit_transform(X_train)\n",
    "X_val_prep = full_pipeline.transform(X_val)\n",
    "X_test_prep = full_pipeline.transform(X_test)\n",
    "print(X_train_prep.getnnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T14:04:12.958886Z",
     "start_time": "2020-08-04T14:01:48.229292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False, random_state=34,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'max_leaf_nodes': [15, 36, 57, 78, 100],\n",
       "                         'n_estimators': [100, 225, 350, 475, 600]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_estimators = [int(i) for i in np.linspace(start=100, stop =600, num = 5) ]\n",
    "max_leaf_nodes = [int(i) for i in np.linspace(start=15, stop=100, num = 5 )]\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_leaf_nodes': max_leaf_nodes\n",
    "}\n",
    "random_reg = RandomForestClassifier(random_state=34)\n",
    "randomForest = GridSearchCV(random_reg, param_grid, cv=5,\n",
    "                           scoring='accuracy',\n",
    "                           return_train_score=True, verbose = 1, n_jobs=-1)\n",
    "\n",
    "randomForest.fit(X_train_prep, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T14:05:00.879550Z",
     "start_time": "2020-08-04T14:04:58.072809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training data:  0.5482987312572087\n",
      "Accuracy on the test set:  0.48385608856088563\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pred_model = randomForest.best_estimator_\n",
    "print(\"Accuracy on the training data: \", pred_model.score(X_train_prep, y_train))\n",
    "print(\"Accuracy on the test set: \", pred_model.score(X_test_prep, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the random forest, I got over a 30% increase in accuracy on the test dataset than using just a decision tree. This was probably becuase a random forest has multiple decision trees. Each of the decision  trees are based on a random sample of the dataset. A single decision tree is prone to overfit but a random forest reduces the chances of it overfitting by aggregating all the results of the many decision trees in the random forest which makes it more robust . With the decision tree there was a very big difference in accuracy between the test data and the training data(37% difference) which is a sign of overfitting. But with the random forest the difference in accuracy between the training data and test data reduces to just 6.5%, so the overfitting has deffinitely reduced"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
